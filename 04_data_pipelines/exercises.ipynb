{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcb807a5",
   "metadata": {},
   "source": [
    "# Data Pipelines: Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e410a",
   "metadata": {},
   "source": [
    "## 1. Implement ETL Pipeline Functions\n",
    "\n",
    "- Create Python functions to perform each step of a simple ETL pipeline for a CSV file:\n",
    "    - **Extract:** Read data from a CSV file into a suitable data structure (e.g., pandas DataFrame).\n",
    "    - **Transform:** Clean or modify the data as needed (e.g., remove missing values, standardize formats, filter rows).\n",
    "    - **Load:** Save the processed data to a new CSV file or another destination.\n",
    "\n",
    "Your functions should each handle one step, and be reusable for different CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6cc1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a00b1",
   "metadata": {},
   "source": [
    "## 2. Modularize Your ETL Pipeline\n",
    "\n",
    "- Refactor your ETL code to improve structure and maintainability:\n",
    "    - Organize your extraction, transformation, and load logic into clearly separated functions.\n",
    "    - Ensure each function only handles one specific part of the process.\n",
    "    - Pass data explicitly between steps (i.e., avoid using global variables).\n",
    "    - Demonstrate the use of your functions by running the pipeline from start to finish on a sample CSV file.\n",
    "\n",
    "This will help make your pipeline easier to read, test, and extend as your data or requirements change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acc76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9fe651",
   "metadata": {},
   "source": [
    "## 3. Add Logging\n",
    "- Enhance the observability of your ETL pipeline by integrating logging into each step (extract, transform, load).\n",
    "- Use Pythonâ€™s built-in `logging` module to record key events, errors, and data quality checks.\n",
    "- Ensure that logs capture the start and end of each step, as well as any important parameters or metrics (e.g., number of records processed).\n",
    "- Logging will help with debugging, monitoring, and maintaining your pipeline in production environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c2da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07026e7c",
   "metadata": {},
   "source": [
    "## 4. Schedule Your ETL Pipeline with Airflow\n",
    "\n",
    "- Build a basic Airflow DAG to orchestrate your ETL pipeline.\n",
    "- Schedule the DAG to run once per day.\n",
    "- Use PythonOperator (or equivalent) to call your extract, transform, and load functions.\n",
    "- Ensure your DAG includes task dependencies and basic documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d5e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8b026",
   "metadata": {},
   "source": [
    "## 5. Error Handling and Retries\n",
    "- Enhance your ETL pipeline by adding robust error handling and automatic retries for each step.\n",
    "- Use try-except blocks to gracefully catch and log errors that occur during extraction, transformation, or loading.\n",
    "- Implement a retry mechanism (with a delay) to handle temporary failures (e.g., file not found, network issues), ensuring your pipeline is more resilient and reliable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbde7f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427b5b33",
   "metadata": {},
   "source": [
    "---\n",
    "### Challenge\n",
    "\n",
    "- Create a configuration file (YAML or JSON) to store and manage your ETL pipeline parameters (such as input/output file paths, filtering options, column names, etc.).\n",
    "- Load these parameters in your ETL script and use them to control pipeline behavior.\n",
    "- This approach makes your pipeline more flexible, reusable, and easier to maintain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f3395",
   "metadata": {},
   "source": [
    "```yaml\n",
    "\n",
    "# config.yaml\n",
    "Your code here\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
