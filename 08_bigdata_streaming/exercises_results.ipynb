{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f15c51",
   "metadata": {},
   "source": [
    "# Big Data & Streaming: Exercise Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9a8d86",
   "metadata": {},
   "source": [
    "## 1. Working with PySpark DataFrames\n",
    "- Initialize a SparkSession, load a CSV file as a DataFrame, and display the inferred schema to understand the data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f2805",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Working with PySpark DataFrames\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySparkDataFrameExample\").getOrCreate()\n",
    "\n",
    "# Load CSV file as DataFrame (inferring schema)\n",
    "df = spark.read.csv('data.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Display inferred schema to understand the data structure\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d60b1fd",
   "metadata": {},
   "source": [
    "## 2. Advanced Filtering and Aggregation\n",
    "\n",
    "- Using PySpark, filter the DataFrame to include only rows where the `value` column exceeds 100.\n",
    "- Then, group the filtered data by the `category` column and compute the count of rows for each group.\n",
    "- Display the resulting grouped counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae7134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'value' > 100, group by 'category', and count rows per group\n",
    "filtered_df = df.filter(df['value'] > 100)\n",
    "grouped_counts = filtered_df.groupBy('category').count()\n",
    "grouped_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142fd5d",
   "metadata": {},
   "source": [
    "## 3. Spark Transformations\n",
    "- Practice chaining multiple DataFrame transformations in PySpark. For example: filter the data based on a condition, select a subset of columns, and then group the data by a categorical column to perform an aggregation. Finally, execute an action (such as `.show()` or `.collect()`) to trigger the computation and view the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain multiple DataFrame transformations in PySpark\n",
    "# Example: filter rows where value > 100, select category and value, group by category and calculate average value, then show results\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "result = (\n",
    "    df.filter(df['value'] > 100)\n",
    "      .select('category', 'value')\n",
    "      .groupBy('category')\n",
    "      .agg(avg('value').alias('avg_value'))\n",
    ")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d12199d",
   "metadata": {},
   "source": [
    "## 4. Kafka Streaming\n",
    "- Implement a Python script that demonstrates real-time messaging with Kafka:\n",
    "    - Create a Kafka producer to send sample messages to a local Kafka topic.\n",
    "    - Create a Kafka consumer to read and print messages from that topic.\n",
    "    - Ensure your script uses `localhost` as the Kafka broker.\n",
    "    - Use separate producer and consumer logic (can be in the same script or separate scripts).\n",
    "    - Test end-to-end message flow by observing that sent messages are received and displayed by the consumer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "\n",
    "# Create producer\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "\n",
    "# Send sample messages to topic\n",
    "for i in range(10):\n",
    "    producer.send('topic', value=f'Message {i}'.encode('utf-8'))\n",
    "\n",
    "# Create consumer\n",
    "consumer = KafkaConsumer('topic', bootstrap_servers='localhost:9092', auto_offset_reset='earliest')\n",
    "\n",
    "# Read and print messages from topic\n",
    "for msg in consumer:\n",
    "    print(msg.value.decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c9b6a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge\n",
    "- Simulate a real-time data stream of random numbers and process it using PySpark Structured Streaming.\n",
    "- Your task: Continuously read the stream, compute and update the running average of the numbers in real time, and output the results as the stream progresses.\n",
    "- This exercise will help you practice handling streaming data, applying windowed aggregations, and working with PySpark's structured streaming APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc008e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Simulate a stream of random numbers using Spark's 'rate' source\n",
    "stream_df = spark.readStream.format('rate').option('rowsPerSecond', 1).load()\n",
    "\n",
    "# Compute the running average using aggregation over all rows seen so far\n",
    "running_avg_df = stream_df.groupBy().agg(avg('value').alias('running_avg'))\n",
    "\n",
    "# Output the updated running average to the console as the stream progresses\n",
    "query = running_avg_df.writeStream.outputMode('complete').format('console').start()\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
