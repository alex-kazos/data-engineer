{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baab89a3",
   "metadata": {},
   "source": [
    "# Cloud & DevOps for Data Engineering: Exercise Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe2326",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Docker Practice\n",
    "- Create a Dockerfile to containerize a basic Python script (e.g., prints \"Hello, Docker!\").\n",
    "- Build the Docker image using your Dockerfile.\n",
    "- Run a container from your image and confirm that the expected output is displayed.\n",
    "- Briefly explain how containerization benefits data engineering workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069fdd60",
   "metadata": {},
   "source": [
    "#### Answer \n",
    "- Create a Dockerfile to containerize a basic Python script (e.g., prints \"Hello, Docker!\").\n",
    "- Build the Docker image using your Dockerfile.\n",
    "- Run a container from your image and confirm that the expected output is displayed.\n",
    "- Briefly explain how containerization benefits data engineering workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf222bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script.py\n",
    "print(\"Hello, Docker!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10408a2d",
   "metadata": {},
   "source": [
    "\n",
    "```dockerfile\n",
    "# Dockerfile\n",
    "FROM python:3.9\n",
    "COPY script.py /app/script.py\n",
    "WORKDIR /app\n",
    "CMD [\"python\", \"script.py\"]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65418427",
   "metadata": {},
   "source": [
    "\n",
    "**Build the Docker image:**\n",
    "```\n",
    "docker build -t myapp .\n",
    "```\n",
    "\n",
    "**Run the container:**\n",
    "```\n",
    "docker run myapp\n",
    "```\n",
    "*Expected output:*\n",
    "```\n",
    "Hello, Docker!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f89a79",
   "metadata": {},
   "source": [
    "\n",
    "**How containerization benefits data engineering workflows:**\n",
    "\n",
    "- **Consistency:** Ensures that the application runs identically across different environments (development, testing, production).\n",
    "- **Isolation:** Separates dependencies and configurations, preventing conflicts between projects.\n",
    "- **Portability:** Makes it easy to move workloads between local machines, cloud, or on-premises environments.\n",
    "- **Scalability:** Facilitates deploying and scaling services quickly in orchestration platforms like Kubernetes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de33cf79",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. CI/CD Pipeline\n",
    "- Build a GitHub Actions workflow that automatically runs your projectâ€™s test suite on every push and pull request.\n",
    "- The workflow should use a suitable environment (e.g., Python, Node, etc.), install dependencies, and execute your tests.\n",
    "- Ensure that failed tests prevent merging, promoting code quality through continuous integration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf501e3",
   "metadata": {},
   "source": [
    "```yaml\n",
    "# yaml file\n",
    "\n",
    "# .github/workflows/ci.yml\n",
    "name: CI\n",
    "on:\n",
    "  push:\n",
    "  pull_request:\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r requirements.txt\n",
    "      - name: Run tests\n",
    "        run: pytest\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205db41d",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Serverless Function (AWS Lambda)\n",
    "- Develop a Python AWS Lambda handler that processes a simple event (e.g., returns a greeting or echoes input).\n",
    "- Demonstrate how to test the Lambda function locally using AWS SAM CLI (or a similar tool).\n",
    "- Explain the key components of the handler (event, context) and best practices for local testing before deploying to AWS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d76f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(event, context):\n",
    "    return {'statusCode': 200, 'body': 'Hello from Lambda!'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6730ac97",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Infrastructure as Code (IaC)\n",
    "\n",
    "- Use Terraform to define and provision cloud resources in a declarative way.\n",
    "- Task: Write a Terraform configuration file that creates an S3 bucket in AWS.\n",
    "- Use a test AWS account and choose a globally unique bucket name.\n",
    "- This exercise helps you practice infrastructure automation and version control for cloud resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0246066",
   "metadata": {},
   "source": [
    "#### Terraform configuration to create an S3 bucket in AWS\n",
    "\n",
    "```hcl\n",
    "provider \"aws\" {\n",
    "  region = \"us-east-1\"\n",
    "}\n",
    "\n",
    "resource \"aws_s3_bucket\" \"test_bucket\" {\n",
    "  bucket = \"dataeng-iac-test-bucket-20240615\" # Replace with a globally unique name\n",
    "  acl    = \"private\"\n",
    "}\n",
    "\n",
    "resource \"aws_s3_bucket_tagging\" \"test_bucket_tags\" {\n",
    "  bucket = aws_s3_bucket.test_bucket.id\n",
    "\n",
    "  tag {\n",
    "    key   = \"Environment\"\n",
    "    value = \"Test\"\n",
    "  }\n",
    "  tag {\n",
    "    key   = \"Purpose\"\n",
    "    value = \"IaC Exercise\"\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e7daa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge\n",
    "- Implement cloud-based monitoring for your data pipeline.\n",
    "- Choose either AWS CloudWatch or GCP Stackdriver for this task.\n",
    "- Set up logging to capture pipeline events and errors.\n",
    "- Configure at least one alert/notification (alarm) for pipeline failures or anomalies.\n",
    "- Briefly document your setup and explain how you would use these tools to troubleshoot and ensure pipeline reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aaa1ef",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "##### Cloud-Based Monitoring with AWS CloudWatch\n",
    "\n",
    "**1. Logging Pipeline Events and Errors**\n",
    "\n",
    "- Integrate Python's `logging` library with CloudWatch Logs using the `watchtower` library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07bf83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import watchtower\n",
    "\n",
    "logger = logging.getLogger(\"etl_pipeline\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(watchtower.CloudWatchLogHandler(log_group=\"etl-pipeline-logs\"))\n",
    "\n",
    "def extract(...):\n",
    "    logger.info(\"Starting extract step\")\n",
    "    try:\n",
    "        # extraction logic\n",
    "        logger.info(\"Extract step completed\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Extract error: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b5dc0c",
   "metadata": {},
   "source": [
    "\n",
    "**2. Configuring CloudWatch Alarm for Pipeline Failures**\n",
    "\n",
    "- Set up a metric filter in CloudWatch Logs:\n",
    "    - Pattern: `{ $.levelname = \"ERROR\" }`\n",
    "    - Metric namespace: `ETLPipeline`\n",
    "    - Metric name: `ErrorCount`\n",
    "- Create a CloudWatch Alarm:\n",
    "    - Threshold: Trigger if `ErrorCount >= 1` in a 5-minute window\n",
    "    - Action: Send notification to an SNS topic (email/SMS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8550ef60",
   "metadata": {},
   "source": [
    "\n",
    "**3. Brief Documentation**\n",
    "\n",
    "- All pipeline logs (info, errors) are streamed to a dedicated CloudWatch log group.\n",
    "- A metric filter tracks error-level events in real-time.\n",
    "- An alarm notifies the data engineering team immediately upon failures.\n",
    "- **Troubleshooting:** Engineers use log search and filtering in CloudWatch Logs to diagnose issues, trace error stack traces, and confirm resolution post-remediation.\n",
    "- **Reliability:** Automated alerting ensures rapid response to failures, minimizing pipeline downtime and improving operational reliability.\n",
    "\n",
    "---\n",
    "\n",
    "*You may use GCP Stackdriver (Cloud Logging/Monitoring) similarly: export logs, set up log-based metrics, and configure alerting policies for error events.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
