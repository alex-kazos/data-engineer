{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baab89a3",
   "metadata": {},
   "source": [
    "# Cloud & DevOps for Data Engineering: Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c479fe",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Docker Practice\n",
    "- Create a Dockerfile to containerize a basic Python script (e.g., prints \"Hello, Docker!\").\n",
    "- Build the Docker image using your Dockerfile.\n",
    "- Run a container from your image and confirm that the expected output is displayed.\n",
    "- Briefly explain how containerization benefits data engineering workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069fdd60",
   "metadata": {},
   "source": [
    "```dockerfile\n",
    "# Dockerfile\n",
    "# Your code here\n",
    "\n",
    "# Build: docker build -t myapp .\n",
    "# Run: docker run myapp\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de33cf79",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. CI/CD Pipeline\n",
    "- Build a GitHub Actions workflow that automatically runs your projectâ€™s test suite on every push and pull request.\n",
    "- The workflow should use a suitable environment (e.g., Python, Node, etc.), install dependencies, and execute your tests.\n",
    "- Ensure that failed tests prevent merging, promoting code quality through continuous integration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf501e3",
   "metadata": {},
   "source": [
    "```yaml\n",
    "# yaml file\n",
    "\n",
    "# .github/workflows/ci.yml\n",
    "# Your code here\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205db41d",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Serverless Function (AWS Lambda)\n",
    "- Develop a Python AWS Lambda handler that processes a simple event (e.g., returns a greeting or echoes input).\n",
    "- Demonstrate how to test the Lambda function locally using AWS SAM CLI (or a similar tool).\n",
    "- Explain the key components of the handler (event, context) and best practices for local testing before deploying to AWS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d76f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6730ac97",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Infrastructure as Code (IaC)\n",
    "\n",
    "- Use Terraform to define and provision cloud resources in a declarative way.\n",
    "- Task: Write a Terraform configuration file that creates an S3 bucket in AWS.\n",
    "- Use a test AWS account and choose a globally unique bucket name.\n",
    "- This exercise helps you practice infrastructure automation and version control for cloud resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0246066",
   "metadata": {},
   "source": [
    "\n",
    "```hcl\n",
    "# hcl file\n",
    "\n",
    "# Your code here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e7daa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge\n",
    "- Implement cloud-based monitoring for your data pipeline.\n",
    "- Choose either AWS CloudWatch or GCP Stackdriver for this task.\n",
    "- Set up logging to capture pipeline events and errors.\n",
    "- Configure at least one alert/notification (alarm) for pipeline failures or anomalies.\n",
    "- Briefly document your setup and explain how you would use these tools to troubleshoot and ensure pipeline reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f5a7c0",
   "metadata": {},
   "source": [
    "#### Your code here"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
