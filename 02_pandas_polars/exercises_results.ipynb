{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas & Polars: Exercise Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a DataFrame\n",
    "- Practice creating a small table of data using both Pandas and Polars. This exercise will help you get comfortable with initializing a DataFrame, explore the syntax for each library, and understand how tabular data is represented in Python using different tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df_pd = pd.DataFrame({'name': ['Alice', 'Bob', 'Charlie'], 'score': [85, 92, 78]})\n",
    "\n",
    "print(df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars\n",
    "df_pl = pl.DataFrame({'name': ['Alice', 'Bob', 'Charlie'], 'score': [85, 92, 78]})\n",
    "\n",
    "print(df_pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read CSV\n",
    "\n",
    "- Practice loading tabular data from a CSV file using both Pandas and Polars. Reading CSVs is a core skill for data engineers, as CSV is one of the most common formats for raw datasets. In this exercise, you'll learn to import data, specify file paths, and handle basic parameters for real-world data ingestion workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "students_pd = pd.read_csv('students.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars\n",
    "students_pl = pl.read_csv('students.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspect Data\n",
    "\n",
    "- Understand your dataset by displaying the first few rows, listing all column names, and summarizing its structure. Use these inspection techniques to quickly assess data quality, identify patterns or anomalies, and plan further cleaning or analysis steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "display(students_pd.head(3))\n",
    "print(\"Pandas columns:\", list(students_pd.columns))\n",
    "\n",
    "# Polars\n",
    "display(students_pl.head(3))\n",
    "print(\"Polars columns:\", students_pl.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filter Data\n",
    "- Practice selecting subsets of your DataFrame by applying conditional filters. For example, extract rows where the 'score' exceeds a specific value, or students belong to a particular class. Mastering filtering helps you focus on relevant records, perform targeted analyses, and prepare data for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "print(students_pd[students_pd['score'] > 80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars\n",
    "print(students_pl.filter(students_pl['score'] > 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Add a Derived Column\n",
    "- Create a new column in your DataFrame that is calculated from existing columns (for example, a 'passed' column that is True if a student's score is greater than or equal to 60, and False otherwise). This exercise will help you practice transforming raw data into more meaningful features, which is a common task in data preparation and feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "students_pd = students_pd.assign(passed=students_pd['score'] >= 60)\n",
    "print(students_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars\n",
    "students_pl = students_pl.with_columns(\n",
    "    (pl.col('score') >= 60).alias('passed')\n",
    ")\n",
    "print(students_pl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Group and Aggregate\n",
    "- Summarize your data by grouping rows based on a specific column and then computing aggregate metrics (e.g., mean, sum, count) for each group. This exercise will help you practice data summarization, which is essential for uncovering trends and patterns in your datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "grouped = students_pd.groupby('passed').agg({'score': ['count', 'mean']})\n",
    "\n",
    "print(grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars\n",
    "print(students_pl.groupby('passed').agg(pl.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handle Missing Data\n",
    "- Detect, analyze, and address missing values in your dataset. Effective handling of nulls is essential for maintaining data quality, enabling accurate downstream analysis, and preventing misleading results. Practice strategies such as identifying missing entries, quantifying their impact, and imputing or removing them as appropriate for your use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "mean_score = students_pd['score'].mean()\n",
    "students_pd['score'] = students_pd['score'].fillna(mean_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars\n",
    "students_pl = students_pl.with_columns(\n",
    "    pl.col('score').fill_null(pl.col('score').mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Merge DataFrames\n",
    "- Combine two DataFrames by joining them on a shared column (key). Merging allows you to integrate related information from different sources, enabling comprehensive analysis and richer datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "emails_pd = pd.DataFrame({'name': ['Alice', 'Bob', 'Charlie'], 'email': ['alice@email.com', 'bob@email.com', 'charlie@email.com']})\n",
    "merged_pd = students_pd.merge(emails_pd, on='name')\n",
    "\n",
    "print(merged_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars\n",
    "emails_pl = pl.DataFrame({'name': ['Alice', 'Bob', 'Charlie'], 'email': ['alice@email.com', 'bob@email.com', 'charlie@email.com']})\n",
    "merged_pl = students_pl.join(emails_pl, on='name')\n",
    "\n",
    "print(merged_pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export DataFrame to CSV\n",
    "\n",
    "- Save your cleaned and transformed DataFrame to a CSV file. Exporting data allows you to persist results, share datasets with collaborators, and use them in other tools or systems. This step is fundamental for data reproducibility and further analysis outside your current environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "output_path = 'output.csv'\n",
    "students_pd.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"CSV written to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars\n",
    "students_pl.write_csv('output.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge\n",
    "- Download a real-world dataset (e.g., from Kaggle or data.gov) and perform exploratory analysis, cleaning, and transformations using both Pandas and Polars.\n",
    "- Compare the performance of Pandas and Polars for common operations (filtering, grouping, aggregations) on this larger dataset.\n",
    "- Document your findings and share insights on when you would prefer one library over the other in practical data engineering scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "rows = 1_000_000\n",
    "data = {'a': np.random.randint(0, 100, rows), 'b': np.random.randint(0, 100, rows)}\n",
    "start = time.time()\n",
    "pd_df = pd.DataFrame(data)\n",
    "pd_df['c'] = pd_df['a'] + pd_df['b']\n",
    "pandas_time = time.time() - start\n",
    "print(f'Pandas time: {pandas_time:.2f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Polars\n",
    "import polars as pl\n",
    "\n",
    "start = time.time()\n",
    "pl_df = pl.DataFrame(data)\n",
    "pl_df = pl_df.with_columns((pl_df['a'] + pl_df['b']).alias('c'))\n",
    "polars_time = time.time() - start\n",
    "print(f'Polars time: {polars_time:.2f}s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(Polars is typically much faster for large datasets)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
