{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f15c51",
   "metadata": {},
   "source": [
    "# Big Data & Streaming: Exercise Results\n",
    "\n",
    "## 1. PySpark DataFrame\n",
    "- Read a CSV file with PySpark and print the schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f2805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.csv('data.csv', header=True)\n",
    "print(df.printSchema())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d60b1fd",
   "metadata": {},
   "source": [
    "## 2. Filtering and Aggregation\n",
    "- Filter rows where a value > 100 and group by a category column, counting rows per group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae7134",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.filter(df['value'] > 100).groupBy('category').count()\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142fd5d",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Spark Transformations\n",
    "- Chain at least two transformations (e.g., filter, select, groupBy) before calling an action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = df.filter(df['value'] > 100).select('category')\n",
    "grouped = filtered.groupBy('category').count()\n",
    "grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d12199d",
   "metadata": {},
   "source": [
    "## 4. Kafka Streaming\n",
    "- Write a Python script to send and consume messages from a Kafka topic (localhost).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "producer.send('topic', b'data')\n",
    "consumer = KafkaConsumer('topic', bootstrap_servers='localhost:9092')\n",
    "\n",
    "for msg in consumer:\n",
    "    print(msg.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c9b6a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge\n",
    "- Process a simulated data stream (e.g., random numbers) in real-time and compute a running average using PySpark Structured Streaming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc008e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "stream_df = spark.readStream.format('rate').option('rowsPerSecond', 1).load()\n",
    "avg_df = stream_df.withColumn('running_avg', avg('value').over())\n",
    "query = avg_df.writeStream.format('console').start()\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
