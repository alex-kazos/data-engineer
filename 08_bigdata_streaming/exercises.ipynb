{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f15c51",
   "metadata": {},
   "source": [
    "# Big Data & Streaming: Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c570ff",
   "metadata": {},
   "source": [
    "## 1. Working with PySpark DataFrames\n",
    "- Initialize a SparkSession, load a CSV file as a DataFrame, and display the inferred schema to understand the data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f2805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d60b1fd",
   "metadata": {},
   "source": [
    "## 2. Advanced Filtering and Aggregation\n",
    "\n",
    "- Using PySpark, filter the DataFrame to include only rows where the `value` column exceeds 100.\n",
    "- Then, group the filtered data by the `category` column and compute the count of rows for each group.\n",
    "- Display the resulting grouped counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae7134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142fd5d",
   "metadata": {},
   "source": [
    "## 3. Spark Transformations\n",
    "- Practice chaining multiple DataFrame transformations in PySpark. For example: filter the data based on a condition, select a subset of columns, and then group the data by a categorical column to perform an aggregation. Finally, execute an action (such as `.show()` or `.collect()`) to trigger the computation and view the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d12199d",
   "metadata": {},
   "source": [
    "## 4. Kafka Streaming\n",
    "- Implement a Python script that demonstrates real-time messaging with Kafka:\n",
    "    - Create a Kafka producer to send sample messages to a local Kafka topic.\n",
    "    - Create a Kafka consumer to read and print messages from that topic.\n",
    "    - Ensure your script uses `localhost` as the Kafka broker.\n",
    "    - Use separate producer and consumer logic (can be in the same script or separate scripts).\n",
    "    - Test end-to-end message flow by observing that sent messages are received and displayed by the consumer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c9b6a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge\n",
    "- Simulate a real-time data stream of random numbers and process it using PySpark Structured Streaming.\n",
    "- Your task: Continuously read the stream, compute and update the running average of the numbers in real time, and output the results as the stream progresses.\n",
    "- This exercise will help you practice handling streaming data, applying windowed aggregations, and working with PySpark's structured streaming APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc008e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
