{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcb807a5",
   "metadata": {},
   "source": [
    "# Data Pipelines: Exercise Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e410a",
   "metadata": {},
   "source": [
    "## 1. Implement ETL Pipeline Functions\n",
    "\n",
    "- Create Python functions to perform each step of a simple ETL pipeline for a CSV file:\n",
    "    - **Extract:** Read data from a CSV file into a suitable data structure (e.g., pandas DataFrame).\n",
    "    - **Transform:** Clean or modify the data as needed (e.g., remove missing values, standardize formats, filter rows).\n",
    "    - **Load:** Save the processed data to a new CSV file or another destination.\n",
    "\n",
    "Your functions should each handle one step, and be reusable for different CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6cc1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract(csv_path):\n",
    "    \"\"\"\n",
    "    Extract step: Reads data from a CSV file into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to the input CSV file.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded data.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def transform(df):\n",
    "    \"\"\"\n",
    "    Transform step: Cleans and modifies the data.\n",
    "    Example: Drops rows with missing values and standardizes column names to lower case.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Transformed DataFrame.\n",
    "    \"\"\"\n",
    "    df_clean = df.dropna()\n",
    "    df_clean.columns = [col.lower() for col in df_clean.columns]\n",
    "    return df_clean\n",
    "\n",
    "def load(df, output_path):\n",
    "    \"\"\"\n",
    "    Load step: Saves the processed DataFrame to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to save.\n",
    "        output_path (str): Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a00b1",
   "metadata": {},
   "source": [
    "## 2. Modularize Your ETL Pipeline\n",
    "\n",
    "- Refactor your ETL code to improve structure and maintainability:\n",
    "    - Organize your extraction, transformation, and load logic into clearly separated functions.\n",
    "    - Ensure each function only handles one specific part of the process.\n",
    "    - Pass data explicitly between steps (i.e., avoid using global variables).\n",
    "    - Demonstrate the use of your functions by running the pipeline from start to finish on a sample CSV file.\n",
    "\n",
    "This will help make your pipeline easier to read, test, and extend as your data or requirements change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(path):\n",
    "    import pandas as pd\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def transform(df):\n",
    "    # Example: remove rows with missing values\n",
    "    return df.dropna()\n",
    "\n",
    "def load(df, path):\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "# Example pipeline run\n",
    "input_path = 'input.csv'\n",
    "output_path = 'output.csv'\n",
    "\n",
    "data = extract(input_path)\n",
    "cleaned_data = transform(data)\n",
    "load(cleaned_data, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9fe651",
   "metadata": {},
   "source": [
    "## 3. Add Logging\n",
    "- Enhance the observability of your ETL pipeline by integrating logging into each step (extract, transform, load).\n",
    "- Use Pythonâ€™s built-in `logging` module to record key events, errors, and data quality checks.\n",
    "- Ensure that logs capture the start and end of each step, as well as any important parameters or metrics (e.g., number of records processed).\n",
    "- Logging will help with debugging, monitoring, and maintaining your pipeline in production environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c2da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s:%(message)s\"\n",
    ")\n",
    "\n",
    "def extract(path):\n",
    "    logging.info(f\"Starting extract from {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    logging.info(f\"Extracted {len(df)} records from {path}\")\n",
    "    return df\n",
    "\n",
    "def transform(df):\n",
    "    logging.info(\"Starting transformation\")\n",
    "    before = len(df)\n",
    "    df = df.dropna()\n",
    "    after = len(df)\n",
    "    logging.info(f\"Removed {before - after} rows with missing values\")\n",
    "    logging.info(\"Transformation complete\")\n",
    "    return df\n",
    "\n",
    "def load(df, path):\n",
    "    logging.info(f\"Starting load to {path}\")\n",
    "    df.to_csv(path, index=False)\n",
    "    logging.info(f\"Loaded {len(df)} records to {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07026e7c",
   "metadata": {},
   "source": [
    "## 4. Schedule Your ETL Pipeline with Airflow\n",
    "\n",
    "- Build a basic Airflow DAG to orchestrate your ETL pipeline.\n",
    "- Schedule the DAG to run once per day.\n",
    "- Use PythonOperator (or equivalent) to call your extract, transform, and load functions.\n",
    "- Ensure your DAG includes task dependencies and basic documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d5e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "def extract():\n",
    "    # Your extract code here\n",
    "    pass\n",
    "\n",
    "def transform():\n",
    "    # Your transform code here\n",
    "    pass\n",
    "\n",
    "def load():\n",
    "    # Your load code here\n",
    "    pass\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"etl_pipeline\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    description=\"A basic ETL pipeline DAG\",\n",
    ") as dag:\n",
    "    \n",
    "    extract_task = PythonOperator(\n",
    "        task_id=\"extract\",\n",
    "        python_callable=extract,\n",
    "    )\n",
    "\n",
    "    transform_task = PythonOperator(\n",
    "        task_id=\"transform\",\n",
    "        python_callable=transform,\n",
    "    )\n",
    "\n",
    "    load_task = PythonOperator(\n",
    "        task_id=\"load\",\n",
    "        python_callable=load,\n",
    "    )\n",
    "\n",
    "    extract_task >> transform_task >> load_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8b026",
   "metadata": {},
   "source": [
    "## 5. Error Handling and Retries\n",
    "- Enhance your ETL pipeline by adding robust error handling and automatic retries for each step.\n",
    "- Use try-except blocks to gracefully catch and log errors that occur during extraction, transformation, or loading.\n",
    "- Implement a retry mechanism (with a delay) to handle temporary failures (e.g., file not found, network issues), ensuring your pipeline is more resilient and reliable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbde7f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "def robust_step(func, *args, retries=3, delay=5, **kwargs):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"{func.__name__} failed: {e} (attempt {attempt+1}/{retries})\")\n",
    "            time.sleep(delay)\n",
    "    raise RuntimeError(f\"{func.__name__} failed after {retries} attempts\")\n",
    "\n",
    "try:\n",
    "    df = robust_step(extract, 'input.csv')\n",
    "    df = robust_step(transform, df)\n",
    "    robust_step(load, df, 'output.csv')\n",
    "except Exception as e:\n",
    "    logging.critical(f\"ETL pipeline failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427b5b33",
   "metadata": {},
   "source": [
    "---\n",
    "### Challenge\n",
    "\n",
    "- Create a configuration file (YAML or JSON) to store and manage your ETL pipeline parameters (such as input/output file paths, filtering options, column names, etc.).\n",
    "- Load these parameters in your ETL script and use them to control pipeline behavior.\n",
    "- This approach makes your pipeline more flexible, reusable, and easier to maintain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f3395",
   "metadata": {},
   "source": [
    "```yaml\n",
    "# config.yaml\n",
    "\n",
    "input:\n",
    "  path: 'input.csv'\n",
    "  columns: \n",
    "    - name\n",
    "    - age\n",
    "    - country\n",
    "    \n",
    "output:\n",
    "  path: 'output.csv'\n",
    "  columns: \n",
    "    - name\n",
    "    - age\n",
    "    - country\n",
    "    - continent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabf8f3c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
