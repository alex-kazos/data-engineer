{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5e850ef",
   "metadata": {},
   "source": [
    "# APIs and Web Scraping: Exercise Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1be8d0",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Public API GET Request\n",
    "- Practice retrieving data from a public REST API using Python.\n",
    "- Make an HTTP GET request to a sample API endpoint (such as a placeholder API).\n",
    "- Parse and print the JSON response.\n",
    "- This exercise builds foundational skills for working with external data sources and understanding HTTP requests in data engineering workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7488b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://jsonplaceholder.typicode.com/posts'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(data)\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacdfa76",
   "metadata": {},
   "source": [
    "## 2. Authentication\n",
    "- Make a request to an API endpoint that requires authentication using an API key (use a publicly available test or demo key).\n",
    "- Add the API key to your request via headers or query parameters as specified by the API documentation.\n",
    "- Print the response and briefly explain where the API key was used in your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Using NASA's public demo API key for authentication\n",
    "api_key = 'DEMO_KEY'\n",
    "url = 'https://api.nasa.gov/planetary/apod'\n",
    "params = {'api_key': api_key}\n",
    "\n",
    "resp = requests.get(url, params=params)\n",
    "print(resp.json())  # The API key is passed as a query parameter in the request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da266d7a",
   "metadata": {},
   "source": [
    "## 3. Parse JSON Response\n",
    "- Retrieve data from an API in JSON format, extract a specific field (e.g., the title of the first post), and print its value. This exercise helps you practice working with JSON data and accessing nested fields using Python dictionaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "resp = requests.get('https://jsonplaceholder.typicode.com/posts')\n",
    "data = resp.json()\n",
    "print(data[0]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657f7939",
   "metadata": {},
   "source": [
    "## 4. Web Scraping\n",
    "- Use BeautifulSoup to extract the title from a provided HTML string.\n",
    "- Practice parsing and navigating HTML documents programmatically.\n",
    "- This exercise will help you become familiar with web scraping fundamentals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5204e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Web Scraping\n",
    "# Use BeautifulSoup to extract the title from a provided HTML string.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '<html><title>Test</title></html>'\n",
    "\n",
    "# Parse the HTML string\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Extract and print the title\n",
    "title = soup.title.text\n",
    "print(\"Page title:\", title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab2e3e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge\n",
    "- Practice ethical web scraping: Choose a popular news website, check its `robots.txt` file to confirm that scraping headlines is allowed, and then write Python code to extract all news headlines from the homepage using `requests` and BeautifulSoup. Print the list of headlines you collect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce4523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Check robots.txt (for demonstration, let's use Hacker News)\n",
    "robots_url = 'https://news.ycombinator.com/robots.txt'\n",
    "robots_resp = requests.get(robots_url)\n",
    "print(\"robots.txt contents:\\n\", robots_resp.text)\n",
    "\n",
    "# Step 2: If allowed (Hacker News allows /), scrape headlines\n",
    "url = 'https://news.ycombinator.com/'\n",
    "resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (ethical scraping for educational purposes)'})\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "headlines = [a.text for a in soup.find_all('span', class_='titleline')]\n",
    "# For legacy compatibility, also try:\n",
    "if not headlines:\n",
    "    headlines = [a.text for a in soup.find_all('a', class_='storylink')]\n",
    "\n",
    "# Clean up and print\n",
    "print([h if isinstance(h, str) else h.get_text(strip=True) for h in headlines])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
